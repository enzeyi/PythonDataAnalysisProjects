{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<left><h1> Extra project (4 points)\n",
    "</h1><left>\n",
    "    \n",
    "<left><h2> Due December 5 11:59pm (Hard deadline)\n",
    "</h2><left>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iris=datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=iris.data\n",
    "# Sepal length, Sepal width, Petal length, Petal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=iris.target\n",
    "# setora, vericolor, virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=iris.data[:,[2,3]]\n",
    "y=iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate integer from 0 to 149\n",
    "arr1=np.arange(0, 150)\n",
    "# Shuffle those integers randomly\n",
    "np.random.shuffle(arr1)\n",
    "arr1\n",
    "# Note that arr1 may chage every time you run this cell due to randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we can obtain the shuffled data set by using the shuffled arrary.\n",
    "Xnew=X[arr1,:]\n",
    "ynew=y[arr1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-1. Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(X_org):\n",
    "    # X_org: shuffled but non-scaled data set: X_org n by m matrix (n: example, m: feature)\n",
    "    # Xmean is the mean value per each column:  Xmean 1 by m matrix\n",
    "    # Xstd is the standard deviation per each column:  Xstd 1 by m matrix\n",
    "    # X_sc: the scaled data set\n",
    "    \n",
    "    # Code starts  \n",
    "    Xmean = X_org.mean(axis=0)\n",
    "    Xstd = X_org.std(axis=0)\n",
    "    # Code ends \n",
    "    \n",
    "    X_sc = (X_org - Xmean) / Xstd\n",
    "    \n",
    "    return X_sc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sc=scaling(Xnew)\n",
    "X_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-2 Split the data sent into the training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the data set, X_sc and ynew . \n",
    "# Code starts \n",
    "Xd = X_sc.T\n",
    "yd = ynew.T\n",
    "# Code ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the dimension of yd for matrix multiplication (np.matmul)\n",
    "yd=np.reshape(ynew,(1,ynew.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are interested in finding 'Label 2'. We use the one-vs-all approach to allow binary classification.\n",
    "# yd takes 1 when the previous yd is 2, otherwise yd takes 0.\n",
    "\n",
    "# Code starts (one or two lines) \n",
    "yd = (yd == 2).astype(int)\n",
    "# Code ends "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we split the data set for training (0-119) and testing (120-149)\n",
    "X_train=Xd[:,:120]\n",
    "X_test=Xd[:,120:]\n",
    "y_train=yd[:,:120]\n",
    "y_test=yd[:,120:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-1 sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    # z is the input value (scalar, vector, matrix)\n",
    "    # s is the output value (scalar, vector, matrix)\n",
    "    # Code starts \n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    # Code ends \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-Initializing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters with zero values\n",
    "def initialize(ndim):\n",
    "    # ndim is the number of features: m \n",
    "    # bias is a scalar: theta_0 \n",
    "    # wgt is a ndim by 1 matrix ([ndim,1], dtype=float): (theta_1,..., theta_m) \n",
    "    # Code starts \n",
    "    wgt = np.zeros((ndim, 1), dtype=float)\n",
    "    bias = 0.0\n",
    "    # Code ends \n",
    "    return wgt, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-Foward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(wgt, bias, X):\n",
    "    # X is the transpose of X_sc, m by n matrix\n",
    "    # You can use np.matmul() for matrix multiplication to calculate z\n",
    "    # aout is a 1 by n matrix\n",
    "    \n",
    "    # Code starts \n",
    "    z = np.matmul(wgt.T, X) + bias\n",
    "    # Code ends \n",
    "    \n",
    "    aout = sigmoid(z)\n",
    "    return aout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-Calculating cost function and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_grad(wgt, bias, X_train, y_train, C):\n",
    "    # X_train, y_train are the transpose of scaled training matrices. (m by n matrices)\n",
    "    # C is the inverse of lambda: regularized parameter.\n",
    "    # cost is the cost function values (J); scalar\n",
    "    # dw is dJ/dtheta, m by 1 matrix\n",
    "    # db is dJ/db\n",
    "    # grads is the dictionary that contains dw and db\n",
    "    m = X_train.shape[0]\n",
    "    aout = forward_propagate(wgt, bias, X_train)\n",
    "    \n",
    "    # Code starts \n",
    "    cost = -(1/m) * np.sum(y_train * np.log(aout) + (1 - y_train) * np.log(1 - aout)) + (C/(2 * m)) * np.sum(wgt**2)\n",
    "    \n",
    "    dz = aout - y_train\n",
    "    dw = (1/m) * np.matmul(X_train, dz.T) + (C/m) * wgt\n",
    "    db = (1/m) * np.sum(dz)\n",
    "    # Code ends \n",
    "    \n",
    "    cost = np.squeeze(np.array(cost))\n",
    "    grads = {\"dw\": dw, \"db\": db}\n",
    "    \n",
    "    return grads, cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-Back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagate(wgt, bias, X_train, y_train, C=100, learn_rate=0.01, print_cost=True):\n",
    "    # learn_rate is the learning rate:\n",
    "    grads, cost = cost_grad(wgt, bias, X_train, y_train, C)\n",
    "    # Retrieve derivatives from grads\n",
    "    dw = grads[\"dw\"]\n",
    "    db = grads[\"db\"]\n",
    "    \n",
    "    # Update the weights w (updated) = w (old) - alpha * dw\n",
    "    # Code starts \n",
    "    wgt = wgt - learn_rate * dw\n",
    "    bias = bias - learn_rate * db\n",
    "    # Code ends \n",
    "    \n",
    "    # Print the cost every 100 training iterations\n",
    "    if print_cost:\n",
    "        print(\"Cost after iteration %f\" % cost)\n",
    "                \n",
    "    # params is the dictionary that stores the weights\n",
    "    params = {\"w\": wgt, \"b\": bias}\n",
    "    grads = {\"dw\": dw, \"db\": db}    \n",
    "    return params, grads, cost   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(wgt, bias, X_test):\n",
    "    # X_test is a m by n matrix for testing\n",
    "    # y_pred is a 1 by n matrix\n",
    "    aout = forward_propagate(wgt, bias, X_test)\n",
    "    \n",
    "    # Code starts \n",
    "    y_pred = (aout > 0.5).astype(int)\n",
    "    # Code ends \n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-Get them algother for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations=500\n",
    "wt, bias = initialize(X_train.shape[0])\n",
    "for i in range(num_iterations):\n",
    "    params, grads, costs = back_propagate(wt, bias, X_train, y_train, C=100, learn_rate=0.01)\n",
    "    wt=params['w']\n",
    "    bias=params['b']\n",
    "    print(wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-1 Make the figures of evolutiong of the cost function (iteration vs cost) in the cases of\n",
    "# C=100, C=0.0001, learn_rate=0.01, learn_rate=1.0, learn_rate=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume you have initialized wgt and bias appropriately\n",
    "ndim = X_train.shape[1]\n",
    "wgt, bias = initialize(ndim)\n",
    "\n",
    "# Define hyperparameters\n",
    "C_values = [100, 0.0001]\n",
    "learn_rate_values = [0.01, 1.0, 0.0001]\n",
    "\n",
    "# Number of iterations\n",
    "num_iterations = 1000\n",
    "\n",
    "# Initialize a dictionary to store costs for each combination of parameters\n",
    "costs_dict = {}\n",
    "\n",
    "# Iterate over combinations of C and learning rates\n",
    "for C in C_values:\n",
    "    for learn_rate in learn_rate_values:\n",
    "        # Initialize parameters\n",
    "        wgt, bias = initialize(ndim)\n",
    "        \n",
    "        # Lists to store costs for plotting\n",
    "        costs = []\n",
    "        \n",
    "        # Training loop\n",
    "        for i in range(num_iterations):\n",
    "            # Perform backpropagation and update parameters\n",
    "            params, grads, cost = back_propagate(wgt, bias, X_train, y_train, C=C, learn_rate=learn_rate, print_cost=False)\n",
    "            wgt = params[\"w\"]\n",
    "            bias = params[\"b\"]\n",
    "            \n",
    "            # Append cost to list\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Store costs in the dictionary\n",
    "        costs_dict[(C, learn_rate)] = costs\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for (C, learn_rate), costs in costs_dict.items():\n",
    "    label = f'C={C}, LR={learn_rate}'\n",
    "    plt.plot(range(num_iterations), costs, label=label)\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Evolution of Cost Function for Different Hyperparameters')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-Predict for the testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=predict(wt, bias, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-2. Show your accuracy for six cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "ndim = X_train.shape[1]\n",
    "wgt, bias = initialize(ndim)\n",
    "\n",
    "# Define hyperparameters\n",
    "C_values = [100, 0.0001]\n",
    "learn_rate_values = [0.01, 1.0, 0.0001]\n",
    "\n",
    "# Number of iterations\n",
    "num_iterations = 1000\n",
    "\n",
    "# Dictionary to store accuracies for each combination of parameters\n",
    "accuracies_dict = {}\n",
    "\n",
    "# Iterate over combinations of C and learning rates\n",
    "for C in C_values:\n",
    "    for learn_rate in learn_rate_values:\n",
    "        # Initialize parameters\n",
    "        wgt, bias = initialize(ndim)\n",
    "        \n",
    "        # Training loop\n",
    "        for i in range(num_iterations):\n",
    "            # Perform backpropagation and update parameters\n",
    "            params, grads, cost = back_propagate(wgt, bias, X_train, y_train, C=C, learn_rate=learn_rate, print_cost=False)\n",
    "            wgt = params[\"w\"]\n",
    "            bias = params[\"b\"]\n",
    "        \n",
    "        # Make predictions on the test set\n",
    "        y_pred = predict(wgt, bias, X_test)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = np.mean(y_pred == y_test)\n",
    "        \n",
    "        # Store accuracy in the dictionary\n",
    "        accuracies_dict[(C, learn_rate)] = accuracy\n",
    "\n",
    "# Display accuracies\n",
    "for (C, learn_rate), accuracy in accuracies_dict.items():\n",
    "    print(f'C={C}, LR={learn_rate}: Accuracy = {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11-Compare your results with sklearn (e.g., C=100, C=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object-Write the code\n",
    "lr=LogisticRegression(C=100,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11-1. Run the cases of C=100 and C=0.001 for training followed by testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "ndim = X_train.shape[1]\n",
    "wgt, bias = initialize(ndim)\n",
    "\n",
    "# Define hyperparameters\n",
    "C_values = [100, 0.001]\n",
    "learn_rate = 0.01\n",
    "num_iterations = 1000\n",
    "\n",
    "# Dictionary to store results\n",
    "results_dict = {}\n",
    "\n",
    "# Iterate over different values of C\n",
    "for C in C_values:\n",
    "    # Training\n",
    "    for i in range(num_iterations):\n",
    "        # Perform backpropagation and update parameters\n",
    "        params, grads, cost = back_propagate(wgt, bias, X_train, y_train, C=C, learn_rate=learn_rate, print_cost=False)\n",
    "        wgt = params[\"w\"]\n",
    "        bias = params[\"b\"]\n",
    "\n",
    "    # Testing\n",
    "    y_pred = predict(wgt, bias, X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    \n",
    "    # Store results in the dictionary\n",
    "    results_dict[C] = {\"wgt\": wgt, \"bias\": bias, \"accuracy\": accuracy}\n",
    "\n",
    "# Display results\n",
    "for C, result in results_dict.items():\n",
    "    print(f'C={C}: Accuracy = {result[\"accuracy\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11-2 Compare the results of prediction (test set) with your code in terms of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming X_train, y_train, X_test, and y_test are defined\n",
    "\n",
    "# Train logistic regression model with C=100\n",
    "model_c100 = LogisticRegression(C=100)\n",
    "model_c100.fit(X_train, y_train)\n",
    "y_pred_c100 = model_c100.predict(X_test)\n",
    "accuracy_c100 = accuracy_score(y_test, y_pred_c100)\n",
    "print(f'Accuracy for C=100 (scikit-learn): {accuracy_c100}')\n",
    "\n",
    "# Train logistic regression model with C=0.001\n",
    "model_c0_001 = LogisticRegression(C=0.001)\n",
    "model_c0_001.fit(X_train, y_train)\n",
    "y_pred_c0_001 = model_c0_001.predict(X_test)\n",
    "accuracy_c0_001 = accuracy_score(y_test, y_pred_c0_001)\n",
    "print(f'Accuracy for C=0.001 (scikit-learn): {accuracy_c0_001}')\n",
    "\n",
    "# Compare with your logistic regression results\n",
    "your_accuracy_c100 = results_dict[100][\"accuracy\"]\n",
    "your_accuracy_c0_001 = results_dict[0.001][\"accuracy\"]\n",
    "\n",
    "print(f'Your accuracy for C=100: {your_accuracy_c100}')\n",
    "print(f'Your accuracy for C=0.001: {your_accuracy_c0_001}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11-3 Visualize of decision boundaries from the sklearn results and your code as you did in the previous labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot decision boundaries\n",
    "def plot_decision_boundary(model, X, y, title):\n",
    "    h = .02  # Step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.show()\n",
    "\n",
    "# Assuming X_train, y_train, X_test, y_test are defined\n",
    "\n",
    "# Train logistic regression model with C=100 using scikit-learn\n",
    "model_c100_sklearn = LogisticRegression(C=100)\n",
    "model_c100_sklearn.fit(X_train, y_train)\n",
    "plot_decision_boundary(model_c100_sklearn, X_train, y_train, 'Decision Boundary - C=100 (scikit-learn)')\n",
    "\n",
    "# Train logistic regression model with C=100 using your code\n",
    "model_c100_your_code = LogisticRegression(C=100)\n",
    "model_c100_your_code.coef_ = results_dict[100][\"wgt\"].T\n",
    "model_c100_your_code.intercept_ = results_dict[100][\"bias\"]\n",
    "plot_decision_boundary(model_c100_your_code, X_train, y_train, 'Decision Boundary - C=100 (your code)')\n",
    "\n",
    "# Train logistic regression model with C=0.001 using scikit-learn\n",
    "model_c0_001_sklearn = LogisticRegression(C=0.001)\n",
    "model_c0_001_sklearn.fit(X_train, y_train)\n",
    "plot_decision_boundary(model_c0_001_sklearn, X_train, y_train, 'Decision Boundary - C=0.001 (scikit-learn)')\n",
    "\n",
    "# Train logistic regression model with C=0.001 using your code\n",
    "model_c0_001_your_code = LogisticRegression(C=0.001)\n",
    "model_c0_001_your_code.coef_ = results_dict[0.001][\"wgt\"].T\n",
    "model_c0_001_your_code.intercept_ = results_dict[0.001][\"bias\"]\n",
    "plot_decision_boundary(model_c0_001_your_code, X_train, y_train, 'Decision Boundary - C=0.001 (your code)')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
