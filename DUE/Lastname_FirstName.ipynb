{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import datetime\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set of 'norne_production_rate_sample.csv'\n",
    "file_path = 'norne_production_rate_sample.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert date string to Panda datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Display the modified DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data from June 2, 2004, to May 1, 2006\n",
    "start_date = '2004-06-02'\n",
    "end_date = '2006-05-01'\n",
    "selected_data = df.loc[(df['Date'] >= start_date) & (df['Date'] <= end_date), ['Date', 'Flow Rate']]\n",
    "\n",
    "# Convert the date data to numpy arrays\n",
    "date_array = selected_data['Date'].to_numpy()\n",
    "flow_rate_array = selected_data['Flow Rate'].to_numpy()\n",
    "\n",
    "# Calculate cumulative sum over timedeltas\n",
    "cumulative_sum = np.cumsum(np.diff(date_array).astype('timedelta64[s]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the time and rate data\n",
    "selected_data['Normalized Date'] = (selected_data['Date'] - selected_data['Date'].min()) / (selected_data['Date'].max() - selected_data['Date'].min())\n",
    "selected_data['Normalized Flow Rate'] = (selected_data['Flow Rate'] - selected_data['Flow Rate'].min()) / (selected_data['Flow Rate'].max() - selected_data['Flow Rate'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperbolic decline function\n",
    "def hyperbolic(t, qi, di, b):\n",
    "    return qi / (np.abs((1 + b * di * t))**(1/b))\n",
    "\n",
    "# Perform curve fitting with the hyperbolic function\n",
    "params, covariance = curve_fit(hyperbolic, normalized_time, normalized_rate, p0=[1, 0.1, 0.5])\n",
    "\n",
    "# Extract the fitted parameters\n",
    "qi_fit, di_fit, b_fit = params\n",
    "\n",
    "# Generate fitted curve using the fitted parameters\n",
    "fitted_curve = hyperbolic(normalized_time, qi_fit, di_fit, b_fit)\n",
    "\n",
    "# Plot the original data and the fitted curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(normalized_time, normalized_rate, label='Original Data')\n",
    "plt.plot(normalized_time, fitted_curve, label='Fitted Curve', color='red')\n",
    "plt.xlabel('Normalized Time')\n",
    "plt.ylabel('Normalized Flow Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De-normalize qi and di\n",
    "qi = qi_fit * (flow_rate_array.max() - flow_rate_array.min()) + flow_rate_array.min()\n",
    "di = di_fit / (date_array.max() - date_array.min())\n",
    "\n",
    "# Forecast gas rate until 1,200 days\n",
    "forecasted_time = np.linspace(normalized_time.min(), 1200, 100)  # 100 points for better resolution\n",
    "forecasted_rate = hyperbolic(forecasted_time, qi_fit, di_fit, b_fit)\n",
    "\n",
    "# Plot the production data with the forecasts (rate and cum. production)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plotting production rate\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.scatter(normalized_time, normalized_rate, label='Original Data')\n",
    "plt.plot(forecasted_time, forecasted_rate, label='Forecasted Rate', color='red')\n",
    "plt.xlabel('Normalized Time')\n",
    "plt.ylabel('Normalized Flow Rate')\n",
    "plt.legend()\n",
    "\n",
    "# Calculate cumulative production\n",
    "cumulative_production = np.cumsum(forecasted_rate) * (date_array.max() - date_array.min()) / 100  # Scaling back to original range\n",
    "\n",
    "# Plotting cumulative production\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(forecasted_time, cumulative_production, label='Cumulative Production', color='green')\n",
    "plt.xlabel('Normalized Time')\n",
    "plt.ylabel('Cumulative Production')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.2 (7 points) Supervised learning** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **Q.2-1 (3 points) Data preparation and preprocessing including scaling** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the training data set of 'facies_training_data.csv'\n",
    "file_path = 'facies_training_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Assuming 'Date' is the name of the column containing date strings\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Now 'Date' column is converted to pandas datetime format\n",
    "\n",
    "# Assuming there's a 'Date' column in the dataset\n",
    "if 'Date' in df.columns:\n",
    "    df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the null values, specifically for 'PE'\n",
    "df = df.dropna(subset=['PE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 'Facies' values as the output values\n",
    "y = df['Facies']\n",
    "\n",
    "# Drop unnecessary columns from the feature vectors\n",
    "# Drop 'Formation', 'Well Name', 'Depth', 'NM_M', 'RELPOS', 'Facies'\n",
    "# Keep 'GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE'\n",
    "X = df.drop(['Formation', 'Well Name', 'Depth', 'NM_M', 'RELPOS', 'Facies'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the feature vectors using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set and test set (80%-20%, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **Q.2-2 (1 point) Logistic Regression** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Logistic Regression with specified hyperparameters\n",
    "lr_model = LogisticRegression(C=0.1, random_state=1)\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform prediction with the test data set\n",
    "y_pred_LR = lr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy by using the following code. \n",
    "# y_test is the output of the test set\n",
    "# y_pred_LR is the prediction of  the test data set, ie, the previous solution.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf = confusion_matrix(y_test, y_pred_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(conf):\n",
    "    total_correct = 0.\n",
    "    nb_classes = conf.shape[0]\n",
    "    for i in np.arange(0,nb_classes):\n",
    "        total_correct += conf[i,i]\n",
    "    acc = total_correct/sum(sum(conf))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **Q.2-3 (1 points) Support Vector Machine** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Support Vector Machine with linear kernel\n",
    "svm_linear_model = SVC(kernel='linear', C=0.1, random_state=1)\n",
    "svm_linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Perform prediction with the test data set (linear kernel)\n",
    "y_pred_svm_linear = svm_linear_model.predict(X_test)\n",
    "\n",
    "# Print the confusion matrix (linear kernel)\n",
    "conf_matrix_linear = confusion_matrix(y_test, y_pred_svm_linear)\n",
    "print(\"Confusion Matrix (Linear Kernel):\\n\", conf_matrix_linear)\n",
    "\n",
    "# Calculate accuracy using the provided function (linear kernel)\n",
    "acc_linear = accuracy(conf_matrix_linear)\n",
    "print(\"Accuracy (Linear Kernel):\", acc_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform prediction with the test data set (Gaussian kernel)\n",
    "y_pred_svm_rbf = svm_rbf_model.predict(X_test)\n",
    "\n",
    "# Print the confusion matrix (Gaussian kernel)\n",
    "conf_matrix_rbf = confusion_matrix(y_test, y_pred_svm_rbf)\n",
    "print(\"\\nConfusion Matrix (Gaussian Kernel):\\n\", conf_matrix_rbf)\n",
    "\n",
    "# Calculate accuracy using the provided function (Gaussian kernel)\n",
    "acc_rbf = accuracy(conf_matrix_rbf)\n",
    "print(\"Accuracy (Gaussian Kernel):\", acc_rbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. 2-4 (1 points) Decision Tree & Random forest** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "dt_model = DecisionTreeClassifier(criterion='gini', max_depth=6, random_state=1)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Perform prediction with the test data set\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "# Print the confusion matrix\n",
    "conf_matrix_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "print(\"Confusion Matrix (Decision Tree):\\n\", conf_matrix_dt)\n",
    "\n",
    "# Calculate accuracy using the provided function\n",
    "acc_dt = accuracy(conf_matrix_dt)\n",
    "print(\"Accuracy (Decision Tree):\", acc_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(criterion='gini', max_depth=4, n_estimators=20, random_state=1, n_jobs=2)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Perform prediction with the test data set\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Print the confusion matrix\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "print(\"Confusion Matrix (Random Forest):\\n\", conf_matrix_rf)\n",
    "\n",
    "# Calculate accuracy using the provided function\n",
    "acc_rf = accuracy(conf_matrix_rf)\n",
    "print(\"Accuracy (Random Forest):\", acc_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. 2-5 (1 point) KNN** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Neighbors Classifier\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Perform prediction with the test data set\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "# Print the confusion matrix\n",
    "conf_matrix_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "print(\"Confusion Matrix (K-Neighbors Classifier):\\n\", conf_matrix_knn)\n",
    "\n",
    "# Calculate accuracy using the provided function\n",
    "acc_knn = accuracy(conf_matrix_knn)\n",
    "print(\"Accuracy (K-Neighbors Classifier):\", acc_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. 3 (3 points) Clustering** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. 3-1 (1.5 point) KMeans Clustering** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the iris data set for clustering. \n",
    "from sklearn import datasets\n",
    "iris=datasets.load_iris()\n",
    "X_train=iris.data[:,[2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform KMeans clustering\n",
    "km = KMeans(n_clusters=3, init='random', n_init=100, tol=1e-04, random_state=0)\n",
    "y_km = km.fit_predict(X_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the followign code for visualization.\n",
    "# y_km is the results vector after clustering.\n",
    "\n",
    "plt.scatter(X_train_std[y_km==0,0],X_train_std[y_km==0,1],c='lightgreen',marker='s',edgecolor='black',s=50,label='Cluster 1')\n",
    "plt.scatter(X_train_std[y_km==1,0],X_train_std[y_km==1,1],c='orange',marker='o',edgecolor='orange',s=50,label='Cluster 2')\n",
    "plt.scatter(X_train_std[y_km==2,0],X_train_std[y_km==2,1],c='lightblue',marker='v',edgecolor='lightblue',s=50,label='Cluster 3')\n",
    "plt.scatter(km.cluster_centers_[:,0],km.cluster_centers_[:,1],c='red',marker='*',edgecolor='red',s=50,label='Centroids')\n",
    "plt.legend(scatterpoints=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. 3-2 (1.5 point) DBSCAN Clustering** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X_train = iris.data[:, [2, 3]]  # Using only the features (petal length and petal width)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Perform DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=6, metric='euclidean')\n",
    "y_dbscan = dbscan.fit_predict(X_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clusters\n",
    "plt.scatter(X_train_std[y_dbscan == 0, 0], X_train_std[y_dbscan == 0, 1], c='lightgreen', marker='s', edgecolor='black', s=50, label='Cluster 1')\n",
    "plt.scatter(X_train_std[y_dbscan == 1, 0], X_train_std[y_dbscan == 1, 1], c='orange', marker='o', edgecolor='orange', s=50, label='Cluster 2')\n",
    "plt.scatter(X_train_std[y_dbscan == 2, 0], X_train_std[y_dbscan == 2, 1], c='lightblue', marker='v', edgecolor='lightblue', s=50, label='Cluster 3')\n",
    "\n",
    "plt.legend(scatterpoints=1)\n",
    "plt.xlabel('Petal Length (standardized)')\n",
    "plt.ylabel('Petal Width (standardized)')\n",
    "plt.title('DBSCAN Clustering on Iris Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.4 (1 point) PCA** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firt run the following code to get another data set.\n",
    "from sklearn.datasets import make_moons\n",
    "Xd, yd=make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "plt.scatter(Xd[:,0],Xd[:,1])\n",
    "\n",
    "db=DBSCAN(eps=0.2,min_samples=5,metric='euclidean')\n",
    "y_db=db.fit_predict(Xd)\n",
    "plt.scatter(Xd[y_db==0,0],Xd[y_db==0,1],c='lightgreen',marker='s',edgecolor='black',s=50,label='Cluster 1')\n",
    "plt.scatter(Xd[y_db==1,0],Xd[y_db==1,1],c='orange',marker='o',edgecolor='orange',s=50,label='Cluster 2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Kernel PCA\n",
    "kpca = KernelPCA(n_components=1, kernel=\"rbf\", gamma=12.5)\n",
    "Xd_kpca = kpca.fit_transform(Xd)\n",
    "\n",
    "# Visualize the results after Kernel PCA\n",
    "plt.scatter(Xd_kpca[y_db == 0], np.zeros_like(Xd_kpca[y_db == 0]), c='lightgreen', marker='s', edgecolor='black', s=50, label='Cluster 1')\n",
    "plt.scatter(Xd_kpca[y_db == 1], np.zeros_like(Xd_kpca[y_db == 1]), c='orange', marker='o', edgecolor='orange', s=50, label='Cluster 2')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Principal Component 1 (Kernel PCA)')\n",
    "plt.title('Kernel PCA Results after DBSCAN Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.5 (2 points) Neural Networks** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code for creating a data set of the XOR problem. \n",
    "np.random.seed(1)\n",
    "x=np.random.uniform(low=-1, high=1, size=(200,2))\n",
    "y=np.ones(len(x))\n",
    "\n",
    "y[x[:,0]*x[:,1]<0]=0\n",
    "# Split the training set into the training set and the validation set\n",
    "x_train=x[:100,:]\n",
    "y_train=y[:100]\n",
    "\n",
    "x_valid=x[100:,:]\n",
    "y_valid=y[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Layer 1: Combined layer (input layer + hidden layer)\n",
    "model.add(Dense(units=3, input_shape=(2,), activation='relu'))\n",
    "\n",
    "# Layer 2: Hidden layer\n",
    "model.add(Dense(units=5, activation='relu'))\n",
    "\n",
    "# Layer 3: Hidden layer\n",
    "model.add(Dense(units=4, activation='relu'))\n",
    "\n",
    "# Layer 4: Output layer\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with specified hyperparameters\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "# Train the model\n",
    "hist = model.fit(x_train, y_train, validation_data=(x_valid, y_valid), epochs=200, batch_size=2, verbose=0)\n",
    "\n",
    "# Visualization\n",
    "history = hist.history\n",
    "fig = plt.figure(figsize=(16, 4))\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "ax = fig.add_subplot(1, 3, 1)\n",
    "plt.plot(history['loss'], lw=4)\n",
    "plt.plot(history['val_loss'], lw=4)\n",
    "plt.legend(['Train loss', 'Validation loss'], fontsize=15)\n",
    "ax.set_xlabel('Epochs', size=15)\n",
    "\n",
    "# Plot Training and Validation Accuracy\n",
    "ax = fig.add_subplot(1, 3, 2)\n",
    "plt.plot(history['binary_accuracy'], lw=4)\n",
    "plt.plot(history['val_binary_accuracy'], lw=4)\n",
    "plt.legend(['Train Acc', 'Validation Acc'], fontsize=15)\n",
    "ax.set_xlabel('Epochs', size=15)\n",
    "\n",
    "# Plot Decision Regions\n",
    "ax = fig.add_subplot(1, 3, 3)\n",
    "plot_decision_regions(X=x_valid, y=y_valid.astype(np.integer), clf=model)\n",
    "ax.set_xlabel(r'$x_1$', size=15)\n",
    "ax.xaxis.set_label_coords(1, -0.025)\n",
    "ax.set_ylabel(r'$x_2$', size=15)\n",
    "ax.yaxis.set_label_coords(-0.025, 1)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
